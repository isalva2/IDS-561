{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 561 Homework 2\n",
    "Isaac Salvador<br>UIN: 669845132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We first instantiate a `pyspark.sql` session to work in a dataframe environment and load the `Amazon_Responded_Oct05.csv` file. The argument `inferSchema=True` allows spark to automatically infer the data types and columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 21:38:06 WARN Utils: Your hostname, Isaacs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.165 instead (on interface en0)\n",
      "23/11/06 21:38:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/06 21:38:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start spark session\n",
    "sc = SparkSession.builder.appName(\"Homework2\").getOrCreate()\n",
    "\n",
    "file_path = \"data/Amazon_Responded_Oct05.csv\"\n",
    "amazon_df = sc.read.csv(file_path, header=True, inferSchema=True).na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we select the relevant columns `\"tweet_created_at\"`, `\"user_screen_name\"`, and `\"user_id_str\"` for further analysis. The field  `\"tweet_created_at\"` is subseqeuntly converted from `string` data type to `TimestampType` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose relevant columns\n",
    "relevant_columns = [\"tweet_created_at\", \"user_screen_name\", \"user_id_str\"]\n",
    "\n",
    "# select relevant columns\n",
    "amazon_analysis_df = amazon_df.select(relevant_columns)\n",
    "\n",
    "# convert \"tweet_created_at\" string column to date column\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# create parsing function to convert from string to timestamp\n",
    "def parse_tweet_created_at(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# create a spark user defined function (UDF)\n",
    "parse_tweet_created_at_udf = udf(parse_tweet_created_at, TimestampType())\n",
    "\n",
    "# apply UDF to \"tweet_created_at function\"\n",
    "amazon_analysis_df = amazon_analysis_df.withColumn(\n",
    "    \"tweet_created_at\",\n",
    "    parse_tweet_created_at_udf(\n",
    "        amazon_analysis_df['tweet_created_at']\n",
    "    )\n",
    ")\n",
    "\n",
    "# remove null values\n",
    "amazon_analysis_filtered_df = amazon_analysis_df.filter(\n",
    "    amazon_analysis_df['tweet_created_at'].isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "### _Step 1_\n",
    "The first step is to `groupby` distinct users and there corresponding user id's and obtain the number of unique days the users have tweeted. We then `filter` to obtain only the users that have tweeted at least 5 times. Per this methodology there were 262 users that tweeted at least 5 times. These results are then exported to csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 21:38:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Daily Active Users: 262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# group users and sum the number of times they tweeted\n",
    "daily_active_users = amazon_analysis_filtered_df\\\n",
    "    .distinct()\\\n",
    "    .groupby([\"user_screen_name\", \"user_id_str\"])\\\n",
    "    .agg(count(\"tweet_created_at\").alias(\"number_of_days\"))\n",
    "\n",
    "# filter for users that have tweeted at least 5 times\n",
    "daily_active_users = daily_active_users.filter(col(\"number_of_days\") >= 5)\n",
    "\n",
    "# select relevant columns\n",
    "daily_active_users = daily_active_users.select(\n",
    "    [\"user_screen_name\", \"user_id_str\"]\n",
    ")\n",
    "\n",
    "print(f\"Number of Daily Active Users: {daily_active_users.count()}\")\n",
    "\n",
    "# export to csv\n",
    "#daily_active_users.write.csv(\"data/HW3_output/daily_active_users.csv\",header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Step 2_\n",
    "To conduct A/B testing on active users, we first load the `experiment.txt` file and convert it to a spark dataframe object. We additionally specify a `schema` such that the column contains `StringType` data types and has the column name `user_id_str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# create custom schema\n",
    "schema = StructType([\n",
    "    StructField(\n",
    "        \"user_id_str\",\n",
    "        StringType(),\n",
    "        True\n",
    "    )\n",
    "])\n",
    "\n",
    "# load experiment.txt\n",
    "file_path = \"data/experiment.txt\"\n",
    "experiment_df = sc.read.csv(file_path, header = False, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next perform a left join on `experiment_from_txt` with `daily_active_users` to generate a new column `\"whether_active\"` using the pyspark function `when`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# perform left join on experiment_from_text with daily_active_users\n",
    "experiment_join_df = experiment_df.join(daily_active_users, 'user_id_str', how=\"left\")\n",
    "\n",
    "\n",
    "# generate new column \"whether_active\" based on null values after left join\n",
    "experiment_join_null_check_df = experiment_join_df.withColumn(\n",
    "    \"whether_active\",\n",
    "    when(col(\"user_screen_name\").isNull(), \"No\").otherwise('yes')\n",
    ")\n",
    "\n",
    "# select only \"user_id_str\" and \"whether_active\"\n",
    "experiment = experiment_join_null_check_df.drop(\"user_screen_name\")\n",
    "\n",
    "# export to csv\n",
    "#experiment.write.csv(\"data/HW3_output/experiment.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the percentage of active users using filter arguments. Out of the 5,000 users in the `experiment.txt` file, 0.22% were active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:======================>                                  (4 + 6) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Active: 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate the percent of active users\n",
    "percent_active = \\\n",
    "    experiment.filter(col(\"whether_active\")==\"yes\").count()/\\\n",
    "    experiment.count()\n",
    "\n",
    "formatted_percentage = \"{:.2f}%\".format(percent_active*100)\n",
    "print(\"Percent Active: \"+formatted_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Step 3_\n",
    "To obtain the desired output we first download the `final_experiment.txt` file and convert to a spark dataframe object. We then drop the `\"whether_active\"` and `user_screen_name` fields which will be repopulated following subsequent join operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load final_experiment.csv file\n",
    "file_path = \"data/final_experiment.csv\"\n",
    "final_experiment_df = sc.read.csv(file_path, header=True)\n",
    "\n",
    "# drop fields\n",
    "final_experiment_df = final_experiment_df.drop(\"user_screen_name\").drop(\"whether_active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use join `final_experiment_df` and the `experiment` dataframe from _Step 2_ to populate the empty row `\"whether_active\"`. We perform another left join to obtain `\"user_screen_name\"` from the original `amazon_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on \"user_id_str\"\n",
    "joined_df = final_experiment_df.join(\n",
    "    experiment, \"user_id_str\", \"left\"\n",
    ")\n",
    "\n",
    "joined_df = joined_df.join(\n",
    "    amazon_analysis_df.select([\"user_id_str\", \"user_screen_name\"]), \"user_id_str\", \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we replace all null values from the resultant joins with the string `\"Not found\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_user_experiment = joined_df.na.fill(\"Not found\")\n",
    "\n",
    "# export to csv\n",
    "#final_user_experiment.write.csv(\"data/HW3_output/final_user_experiment.csv\", header=True)\n",
    "\n",
    "# end spark session\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
