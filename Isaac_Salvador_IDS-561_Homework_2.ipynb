{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 561 Homework 2\n",
    "Isaac Salvador<br>UIN: 669845132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The first task prior to data manipulation is importing the necessary `spark` packages and the _**Amazon_Responded_Oct05.csv**_ file as an `RDD` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/16 19:27:29 WARN Utils: Your hostname, Isaacs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.12 instead (on interface en0)\n",
      "23/10/16 19:27:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/16 19:27:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "conf = SparkConf().setAppName(\"IDS561_HW2\").set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "# create initial rdd with raw csv file data\n",
    "rdd = sc.textFile(\"data/Amazon_Responded_Oct05.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `take()` method within in the `head()` function to see the first 5 records of the rdd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_str,tweet_created_at,user_screen_name,user_id_str,user_statuses_count,user_favourites_count,user_protected,user_listed_count,user_following,user_description,user_location,user_verified,user_followers_count,user_friends_count,user_created_at,tweet_language,text_,favorite_count,favorited,in_reply_to_screen_name,in_reply_to_status_id_str,in_reply_to_user_id_str,retweet_count,retweeted,text\n",
      "\n",
      "'793270689780203520',Tue Nov 01 01:57:25 +0000 2016,SeanEPanjab,143515471,51287,4079,False,74,False,\"Content marketer; Polyglot; Beard aficionado; Sikh. Persian, Catalan, French, Spanish. You'll find lol in the interstitial lulls of my tweets.\",غریب الوطن,False,1503,850,Thu May 13 17:43:52 +0000 2010,en,@AmazonHelp Can you please DM me? A product I ordered last year never arrived.,0,False,AmazonHelp,,85741735,0,False,\n",
      "'793281386912354304',Tue Nov 01 02:39:55 +0000 2016,AmazonHelp,85741735,2225450,11366,False,796,False,We answer Amazon support questions 7 days a week. Support available in English / Deutsch / Español / Português / Français / Italiano / 日本語,,True,149569,53,Wed Oct 28 04:17:54 +0000 2009,en,\"@SeanEPanjab I'm sorry, we're unable to DM you. Was this order purchased on https://t.co/nUUp5MLhYl, or one of our other sites? ^CL\",0,False,SeanEPanjab,7.932706897802035e+17,143515471,0,False,\n",
      "'793501578766319616',Tue Nov 01 17:14:53 +0000 2016,SeanEPanjab,143515471,51287,4079,False,74,False,\"Content marketer; Polyglot; Beard aficionado; Sikh. Persian, Catalan, French, Spanish. You'll find lol in the interstitial lulls of my tweets.\",غریب الوطن,False,1503,850,Thu May 13 17:43:52 +0000 2010,en,@AmazonHelp It was purchased on https://t.co/gPWnuSkbwC.,0,False,AmazonHelp,7.932813869123543e+17,85741735,0,False,@AmazonHelp It was purchased on https://t.co/gPWnuSkbwC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def head(rdd, n: int=5) -> None:\n",
    "    for i in rdd.take(n):\n",
    "        print(i)\n",
    "\n",
    "head(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now apparent that there are two issues with the `rdd`. Spark is not context-aware and the headers appear as an instance in the data, and there are empty rows scattered throughout the `rdd`. We can make use of the `.filter()` method to remove the errant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first string in rdd\n",
    "header = rdd.first()\n",
    "column_names = header.split(\",\")\n",
    "\n",
    "rdd1 = rdd\\\n",
    "    .filter(lambda x: x != header)\\\n",
    "    .filter(lambda x: x != '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also conveniently show the indices and column names as a byproduct of this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\tcolumn name\n",
      "-------------------\n",
      "0\tid_str\n",
      "1\ttweet_created_at\n",
      "2\tuser_screen_name\n",
      "3\tuser_id_str\n",
      "4\tuser_statuses_count\n",
      "5\tuser_favourites_count\n",
      "6\tuser_protected\n",
      "7\tuser_listed_count\n",
      "8\tuser_following\n",
      "9\tuser_description\n",
      "10\tuser_location\n",
      "11\tuser_verified\n",
      "12\tuser_followers_count\n",
      "13\tuser_friends_count\n",
      "14\tuser_created_at\n",
      "15\ttweet_language\n",
      "16\ttext_\n",
      "17\tfavorite_count\n",
      "18\tfavorited\n",
      "19\tin_reply_to_screen_name\n",
      "20\tin_reply_to_status_id_str\n",
      "21\tin_reply_to_user_id_str\n",
      "22\tretweet_count\n",
      "23\tretweeted\n",
      "24\ttext\n"
     ]
    }
   ],
   "source": [
    "# print indices and columns of rdd1\n",
    "print(\"index\\tcolumn name\")\n",
    "print(\"-------------------\")\n",
    "\n",
    "for i, column in enumerate(column_names):\n",
    "    print(f\"{i}\\t{column}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make the assumption that valid records in the dataset correspond to those whose whose lists created by the `split()` method contain 25 elements, as shown in the above column name summary. We can also use this operation to extract the six relevant columns `id_str`, `tweet_created_at`, `user_verified`, `favorite count`, `retweet_count`, and `text_` needed for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id_str', 'tweet_created_at', 'user_verified', 'favorite_count', 'retweet_count', 'text_']\n",
      "[\"'793502854459879424'\", 'Tue Nov 01 17:19:57 +0000 2016', 'True', '0', '0', '@SeanEPanjab Please give us a call/chat so we can look into this order for you: https://t.co/hApLpMlfHN. ^HB']\n",
      "[\"'793513446633533440'\", 'Tue Nov 01 18:02:03 +0000 2016', 'True', '0', '0', \"@SeanEPanjab I'm not able to access account info here. Please reach out by Phone/Chat so we can look at this: https://t.co/EKXRLsnxJu ^GL\"]\n",
      "[\"'793299404975247360'\", 'Tue Nov 01 03:51:31 +0000 2016', 'False', '0', '0', \"@JeffBezos @amazonIN @AmazonHelp Tring...Tring...Tring Who's There? Your Suffering Customers from India Mr. Bezos...Get Up and Help!\"]\n",
      "[\"'793407430344310785'\", 'Tue Nov 01 11:00:46 +0000 2016', 'False', '0', '0', '@AmazonHelp How many times do you want to write back to you guys??? Check the complaints through email regarding  Order 171-1338898-5999507.']\n",
      "[\"'793423313674571776'\", 'Tue Nov 01 12:03:53 +0000 2016', 'True', '0', '0', '@aakashwangnoo Hi! We have responded to you here: https://t.co/v4YVCa3rff ^SG(1/2)']\n"
     ]
    }
   ],
   "source": [
    "relevant_indices = [0,1,11,17,22,16]\n",
    "\n",
    "rdd2 = rdd1\\\n",
    "    .filter(lambda x: len(x.split(\",\")) == 25)\\\n",
    "    .map(lambda x: [x.split(\",\")[i] for i in relevant_indices])\n",
    "\n",
    "print([column_names[i] for i in relevant_indices])\n",
    "head(rdd2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Step 1_\n",
    "To remove records where `\"user_verfied\"` is `\"False\"`, we use the `filter()` method accordingly. Subseqeuntly, the `map()` function is used to remove the `\"user_verified\"` column and convert the `\"favorite_count\"` and `\"retweet_count\"` columns to `int()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of verified records: 100965\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2\\\n",
    "    .filter(lambda x: x[2] == \"True\")\\\n",
    "    .map(lambda x: [x[0], x[1], int(x[3]), int(x[4]), x[5]] )\n",
    "\n",
    "print(f\"Number of verified records: {rdd3.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Step 2_\n",
    "In order to group by `\"tweet_created_at\"`, we first perform string manipulation to extract the month and date in the form `\"MMM 00\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.map(\n",
    "    lambda x: [x[0], x[1][4:10], *x[2:]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the number of tweets created per day, we apply the following `rdd` methods:\n",
    "1. `groupBy()` – Records are grouped by the modified `tweet_created_at` column.\n",
    "2. `map()` – Records are mapped to date and count of records per date.\n",
    "3. `collect()` – `rdd4` is collected as a regular python object for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan 11 had 893 tweets, the highest number of tweets in the dataset.\n"
     ]
    }
   ],
   "source": [
    "tweet_dates = rdd4\\\n",
    "    .groupBy(\n",
    "        # use groupBy() to group by tweet_created_at\n",
    "        lambda x: x[1]\n",
    "    )\\\n",
    "    .map(\n",
    "        # use map() to collect dates and number of corresponding records\n",
    "        lambda x: (x[0], len(list(x[1])))\n",
    "    ).collect()\n",
    "\n",
    "# obtain the date with the most tweets\n",
    "biggest_day, most_tweets = max(tweet_dates, key = lambda x: x[1])\n",
    "\n",
    "print(f\"{biggest_day} had {most_tweets} tweets, the highest number of tweets in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Step 3_\n",
    "#### Part 1: Sum of `\"favoritecount\"` and `\"retweet_count\"`_\n",
    "To get the sum of `\"favoritecount\"` and `\"retweet_count\"` for each tweet on `biggest_day`, we perform the following operations on `rdd4`:\n",
    "1. `filter()` – Filter tweets to those who were created on `biggest_day`\n",
    "2. `map()` – Map `rdd5` to a set of key-value pairs that corresponds to `\"id_str\"` and sum of `favoritecount` and `retweet_count`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'819265252453941249'\", 0]\n",
      "[\"'819006776070770690'\", 0]\n",
      "[\"'819154757193498624'\", 1]\n",
      "[\"'819194260243312640'\", 0]\n",
      "[\"'819251153112367105'\", 0]\n",
      "[\"'819235424476495872'\", 1]\n",
      "[\"'819167359734874112'\", 0]\n",
      "[\"'819045790878408704'\", 0]\n",
      "[\"'819048545508622336'\", 0]\n",
      "[\"'819256450543472640'\", 0]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4\\\n",
    "    .filter(\n",
    "        lambda x: x[1] == biggest_day\n",
    "    )\\\n",
    "    .map(\n",
    "        lambda x: [x[0], x[2]+x[3]]\n",
    "    )\n",
    "    \n",
    "head(rdd5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: _Text_ content of the top 100 tweets\n",
    "To obtain the text contents of the top 100 tweets, we first construct `rdd6` by using the `groupBy` method on `rdd5`, sorted by the sum of `\"favoritecount` and `\"retweet_count_\"`. We then perform a `leftOuterJoin()` on `rdd6` with `rdd4` using the key `\"id_str\"`. Finally, a `map()` method is used to extract the text from the resultant joined rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@darryl_edison Hello! These products were never sold on Amazon.in. We have escalated this (1/2) ^HK\n",
      "@tonini30 We're very sorry for the multiple tries it took to resolve your issue! Thank you for the update! ^CC\n",
      "@MrsKatEdd My apologies for this. Are you able to cancel it here?: https://t.co/EMoca4Sfya ^PK\n",
      "@betagirl96 Getting your orders to you by the estimated delivery date is our priority. ^EM\n",
      "@deepmahan Hi there! We're sorry you don't have your order yet. Did you happen to receive any correspondence on the delay? ^YP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd5.sortBy(\n",
    "    lambda x: x[1], ascending=False\n",
    ")\n",
    "\n",
    "top_100_tweets = rdd6\\\n",
    "    .leftOuterJoin(\n",
    "        rdd4.map(\n",
    "            lambda x: [x[0], x[-1]]\n",
    "        )\n",
    "    )\\\n",
    "    .map(\n",
    "        lambda x: x[1][1]\n",
    "    ).take(100)\n",
    "    \n",
    "for tweet in top_100_tweets[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Word Frequency Counts\n",
    "We can finally iterate through `top_100_tweets` to obtain the word (token) freqeuncy counts of the top 100 tweets that occurred on January 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('to', 66)\n",
      "('the', 62)\n",
      "('you', 44)\n",
      "('here:', 28)\n",
      "('this', 27)\n",
      "('your', 27)\n",
      "('for', 26)\n",
      "('us', 22)\n",
      "('sorry', 21)\n",
      "('can', 19)\n"
     ]
    }
   ],
   "source": [
    "# empty dict to fill with word frequency counts\n",
    "word_freq_counts = {}\n",
    "\n",
    "# loop through top 100 tweets\n",
    "for tweet in top_100_tweets:\n",
    "    # loop through tokens in tweets\n",
    "    for token in tweet.split():\n",
    "        # add token if not in dict\n",
    "        if token not in word_freq_counts.keys():\n",
    "            word_freq_counts[token] = 1\n",
    "        # increase count if token in dict\n",
    "        else:\n",
    "            word_freq_counts[token] += 1\n",
    "\n",
    "# sort by frequency\n",
    "sorted_word_freq_counts = sorted(word_freq_counts.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# display\n",
    "for entry in sorted_word_freq_counts[:10]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the file `find_text.csv` as an `RDD` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_text_rdd = sc.textFile(\"data/find_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the function `key_val_generator()` to convert every row in `find_text_rdd` except for the header into key-value pairs with placeholder values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_str,text\n",
      "[\"'793270689780203520'\", None]\n",
      "[\"'793281386912354304'\", None]\n",
      "[\"'793299404975247360'\", None]\n",
      "[\"'793301295255945216'\", None]\n"
     ]
    }
   ],
   "source": [
    "def key_val_generator(index, iterator):\n",
    "    for i, line in enumerate(iterator):\n",
    "        if i == 0:\n",
    "            yield line  # keep the first element as is\n",
    "        else:\n",
    "            yield [line[0:-1], None]  # remove commas from the rest of the elements\n",
    "\n",
    "\n",
    "find_text_rdd2 = find_text_rdd.mapPartitionsWithIndex(\n",
    "    key_val_generator\n",
    "    )\n",
    "\n",
    "head(find_text_rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `rdd2` we extract `id_str` and the `text` columns for a subseqeunt `leftOuterJoin()` with `find_text_rdd2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd2.map(\n",
    "    lambda x: x[0::5]\n",
    ")\n",
    "\n",
    "find_text_rdd3 = find_text_rdd2\\\n",
    "    .leftOuterJoin(rdd6)\\\n",
    "    .sortByKey(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the results, we construct a second generator function that returns the original head of `find_text_rdd` and cleans up the content following the left join opertation. Note that some `id_str` keys do not match up with any records from `rdd6`, a possible consequence from the initial assumptions filtering `rdd2` from `rdd1`.\n",
    "\n",
    "`find_text_complete` is then saved to the local drive using the `saveAsTextFile` method and the spark session is ended using `stop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdd_cleanup(index, iterator):\n",
    "    for i, line in enumerate(iterator):\n",
    "        if i == 0:\n",
    "            yield \"id_str,text\" # edit header string\n",
    "        else:\n",
    "            yield line[0]+\",\"+ (line[1][1] if line[1][1] != None else \"None\")\n",
    "\n",
    "find_text_complete = find_text_rdd3.mapPartitionsWithIndex(\n",
    "    rdd_cleanup\n",
    ")\n",
    "\n",
    "# write to csv\n",
    "find_text_complete.coalesce(1).saveAsTextFile(\"data/find_text\")\n",
    "\n",
    "# rename output\n",
    "import os\n",
    "os.rename(\"data/find_text/part-00000\", \"data/find_text/find_text.csv\")\n",
    "\n",
    "# end session\n",
    "sc.stop() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
